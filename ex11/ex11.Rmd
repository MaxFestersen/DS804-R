---
title: 'DS804: Exercise 11: Group Project 1: Clustering'
author: Alexander Dernild, Christian R. Andersen, Johannes Sørensen, Mathias P.
  Larsen & Max F. Hansen
date: "28/4/2021"
output: pdf_document
---
```{r Imports, echo = FALSE, message = FALSE}
# Imports ---------------------------------------
library(tidyverse) # For convenience, plots, tables and probably some other stuff.
library(cluster)
library(factoextra)
library(SentimentAnalysis) # for sentiment analysis
library(dbscan)
library(class)
library(caret)
library(plyr)
library(fpc)
library(e1071)
library(gridExtra)
# Source ----------------------------------------
source("../cluster_quality.R") # Functions for data analysis

```

```{r 0.0.1 Seeding, echo = FALSE, message = FALSE}
set.seed(42069) # Set seed to remove randomness when replicating results

```

# **DS804 Assignment 1 - Clustering**
28/04/2021  

## Created by  

* Alexander Ibsen Dernild, alder17  
* Christian Rohde Andersen, chran20  
* Johannes Christian Sørensen, josoe20  
* Mathias Profft Larsen, matla20  
* Max Festersen Hansen, maxfh20  

## 1. Introduction
For this project, we have chosen a dataset in the "Shape sets" at [https://cs.joensuu.fi/sipu/datasets/]. In this report we will explain why we chose this dataset, which methods we have chosen to analyze the data and explain the results of our analysis. At last we will be making a comparison of the methods and see how it can be upscaled to larger datasets.


### 1.1 Zahn's Compound dataset
For this exercise, we needed shape data set. We choose to use the compound
dataset, from C.T. Zahn, **IEEE Transactions on Computers**, 1971.
The compound data was chosen for... reasons.<!-- XX TODO: expand reason.  -->

```{r 1.1.1 Loading-dataset, echo = FALSE, message = FALSE}
# Data load
compound.df <- read.csv("Compound.txt", sep = "\t", header = F)
# Adding lables
names(compound.df) <- c("x", "y", "class")

compound.df <- compound.df %>% 
  mutate(class = ifelse(class == 1, "Rain", class)) %>% 
  mutate(class = ifelse(class == 2, "Corona-virus", class)) %>% 
  mutate(class = ifelse(class == 3, "Left-eye", class)) %>% 
  mutate(class = ifelse(class == 4, "Right-eye", class)) %>% 
  mutate(class = ifelse(class == 5, "Lips", class)) %>% 
  mutate(class = ifelse(class == 6, "Tounge", class))

compound.df$class <- factor(compound.df$class)

```

```{r 1.1.2 Plotting-initial-data, warning=FALSE, echo=FALSE}
ggplot(compound.df, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Compound Data") +
  theme_light()

```
Above the data can be seen, colored by class. The clases is named 1-8.
The data had 8 classes, thus 8 colors are used.


```{r 1.1.3 Normalizing-data, warning=FALSE, echo=FALSE}
norm.compound <- scale(compound.df[1:2]) %>% 
  data.frame()
norm.compound$class <- compound.df$class
td <- norm.compound[1:2] # Test data - the data we test on

ggplot(norm.compound, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Compound Data normalized") +
  theme_light()


fviz_nbclust(norm.compound[1:2], kmeans, method='silhouette') # elbow plot

```
We normalize the data to be closer to zero (centered zero). This is done to normalize the weight and remove bias from large numbers.

## 2 Methods  


### 2.1 Algorithms and implementation  
<!-- XX TODO: explain witch algorithms and implementations that were used -->



## 3 Analysis of the Methods

### 3.1 k-means
With k-means we expected it would be able to create good clusters
for the eyes and tongue, since they have a clear mean. We would expect it to
have some trouble with the lips, the rain and the virus,
since the cluster-means overlap with other clusters.

```{r 3.1.1 K-means testing methods, warning=FALSE, echo=FALSE}
set.seed(42069) # Set seed to remove randomness when replicating results
c_macqueen <- kmeans(td, 6, iter.max = 20, algorithm = "MacQueen")
c_forgy <- kmeans(td, 6, iter.max = 20, algorithm = "Forgy")
c_har_won <- kmeans(td, 6)

test <- data.frame(macqueen = c_macqueen$cluster,
                   forgy = c_forgy$cluster,
                   har_won = c_har_won$cluster,
                   class = norm.compound$class)

macqueen <- table(test$macqueen, test$class, dnn = c("MacQueen", "Class"))
forgy <- table(test$forgy, test$class, dnn = c("Forgy", "Class"))
har_won <- table(test$har_won, test$class, dnn = c("Harting-Wong", "Class"))

macqueen
forgy
har_won

cluster_report(macqueen, cap = "MacQueen")
cluster_report(forgy, cap = "Forgy")
cluster_report(har_won, cap = "Hartigan Wong")

si <- silhouette(c_har_won$cluster, dist(norm.compound))
silhouette_score <- mean(si[,3])

```

```{r 3.1.2 k-means-visualization, warning=FALSE, echo=FALSE}
norm.compound$macqueen <- factor(test$macqueen)
norm.compound$forgy <- factor(test$forgy)
norm.compound$har_won <- factor(test$har_won)

plot_stuff <- function(df, predicted, cap) {
  ggplot(df, aes(x, y, shape = class, color = predicted)) +
    geom_point() +
    labs(title = cap) +
    theme_light()
}

plot_stuff(norm.compound, norm.compound$macqueen, "Macqueen")
plot_stuff(norm.compound, norm.compound$forgy, "Forgy")
plot_stuff(norm.compound, norm.compound$har_won, "Hartigan Wong")

```
Using the k-means algorithms: Macqueen, Loyd-Forgy and Hartigan-Wong. We found
Hartigan-Wong was the most accurate when looking at the accuracy of estimates,
and mean macro average. They are visualized above.
Regardless of method, it seems the algorithms are having a hard time distinguishing:
1. Lips & Tongue
2. Left-eye & Right-eye
3. Rain
4. Corona-virus
5. Corona-virus & Rain

It is worth noting that Hartigan-Wong gets Right-eye more wrong with no clustering,
and has a harder time handling Rain and Corona-Virus,
using more clusteres there than the others. It does get Lips and Left-eye right.

It has the best visual result, with the creation og a Mouth, Eyes and multicolored
Scenery.

From what we predicted, we correctly predicted some of the issues with kmeans.
Additionally we were surprised to see Tongue being split in some cases.
The eyes were also never classified correctly, with them being clustered close
together and sometimes together with Lips and Tongue.
<!--XX TODO: Ret fejl i de beskrivelser Max har skrevet ovenfor -->

### 3.2 knn
<!-- Mathias: XX TODO: perform analysis -->
to use KNN we need to have noise free dataset and all attributes of the dataset need to belong to a class.
we start by choosing the value of K we square root the total number of data points available in the dataset, generally it is a good idea for k to be a odd number.

```{r 3.2.1 KNN preproccesing, warning=FALSE, echo=FALSE}
set.seed(42069)
#getting data ready
ran <- sample(1:nrow(compound.df), 0.80* nrow(compound.df))
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
knn_compound_norm <- as.data.frame(lapply(compound.df[,c(1,2)], nor))
norm.test <- cbind(knn_compound_norm, compound.df$class)
names(norm.test) <- c("x", "y", "class")

#using all data, test and training are the same
compound_train <- knn_compound_norm #all of data
compound_test <- knn_compound_norm #all of data
compound_target_category <- compound.df[,3]#all data
compound_test_category <- compound.df[,3]#all data

#using actually training data and test data
compound_train <- knn_compound_norm[ran,] #train data values
compound_test <- knn_compound_norm[-ran,] #test data values
compound_target_category <- compound.df[ran,3] #train data category
compound_test_category <- compound.df[-ran,3] #test data category


#when K=1: potential underfit
#when K approaches total number of rows:potential overfit
# K should be an odd number
#so we should cross validate
sqrt(399)
#KNN algorithm where key is equal to square root of total number of rows
predictions <- knn(compound_train,compound_test,cl=compound_target_category,use.all = TRUE ,k=1)

```

confussion matrix of KNN with K=19 and a 80% training set, 20% test set
```{r 3.2.2 KNN confussion matrix/predictions, warning=FALSE, echo=FALSE}
#confussion matrix
set.seed(100440)
tab <- table(predictions,compound_test_category)
#tab

#accuracy of test set/model
#accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

#predictions
predictions

#confusion Matrix with accuracy
confusionMatrix(predictions ,compound_test_category)

#CrossTable
library(gmodels)
#not sure we need this
#CrossTable(y=compound_test_category,x=predictions,prop.chiq=FALSE)

#error rate
mean(compound_test_category != predictions)

```
plot that displaying the accuracy of the KNN algorithm given the K=i
```{r 3.2.3 KNN optimal k based on test/training set, warning=FALSE, echo=FALSE}
#displaying the accuracy of the KNN algorithm given the K=i

i=1
k.optm=1
for (i in 1:40){
knn.mod <- knn(train=compound_train, test=compound_test,cl=compound_target_category,k=i)
k.optm[i] <- 100* sum(compound_test_category==knn.mod)/NROW(compound_test_category)
k=i
cat(k, '=',k.optm[i],'\n')
}

plot(k.optm, type="b", xlab="K-Value", ylab="Accuracy level")

```


Plotting predicted test as a plot(Remember, this only shows the test data which is 20% of the total dataset)
```{r 3.2.4 KNN plot based test/training set, warning=FALSE, echo=FALSE}
#plot
set.seed(100440)
plot.df = data.frame(compound_test, predicted = predictions)
plot.df1 = data.frame(x = plot.df$y, 
                      y = plot.df$x, 
                      predicted = plot.df$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]

boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)

border_plot<-ggplot(plot.df, aes(y, x, color = predicted, fill = predicted)) +
  geom_point(size = 2)
#turn the graph so it is facing the right way
border_plot<-border_plot+ coord_flip()
#plot without border
border_plot+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))
#plot with border
border_plot+geom_polygon(data = boundary, aes(x,y), alpha = 0.5)+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))

```

Wee can use cross validation to get a more optimal k, by using the whole data set and split it up to several pieces, this way our training set will be larger, and we avoid the disadvantage of testing our whole data set in one go(to avoid more bias).
So we split the data up in 10, so 10*10% of the dataset will be tested and the average accuracy and the optimal K would be calculated.
So we will get the Get the confusion matrix to see accuracy value and other parameter values
```{r 3.2.5 KNN 10 fold cross validation, warning=FALSE, echo=FALSE}

set.seed(100440)

trControl <- trainControl(method  = "cv",
                          number  = 10)

cross_fit <- train(class ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:30),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = norm.test)

confusionMatrix(cross_fit)
#Cross validation best K as a plot
plot(cross_fit)
#Cross validation best K
cross_fit
#plot of ROC(repeated Cross-validation)
plot(cross_fit, print.thres = 0.5, type="S")

```
Cross validation prediction tested on the dataset(remember, Kappe is the expected accuracy)

```{r 3.2.5 KNN 10 fold cross validation_2, warning=FALSE, echo=FALSE}
set.seed(100440)


knnPredict <- predict(cross_fit,newdata = norm.test )
confusionMatrix(knnPredict,compound.df$class)


#Cross validation prediction
#knnPredict
#Get the confusion matrix to see accuracy value and other parameter values

```



### 3.3 OPTICS
```{r 3.3.1 Optics-setup, warning=FALSE, echo=FALSE}
set.seed(2)
dat <- compound.df[1:2] # data

opt_clust <- optics(dat, minPts = 7)
plot(opt_clust, sub = "minPts = 7")

```

When looking at the "valleys" in the above reachability plot, it seems as if there are 5 classes with somewhat different density, however, there actually are 6 as we know from the data description.

Comparing the reachability plot to the clustering order plot below, we can see that the clustering starts somewhere between the rain and the right eye, going to the left eye and moving further on to the mouth and tounge. Then it goes back into the rain, finds the corona-virus and finishes with the rain, which is most likely the bump at the end having the lowest density.

```{r 3.3.1 Optics-order, warning=FALSE, echo=FALSE}
plot(dat, col = "grey", main = "Clustering order", sub = "minPts = 7")
polygon(dat[opt_clust$order,])

```

**Extracting clusters**

To extract clusters from the OPTICS algorithm there are two methods, using DBSCAN with a fixed epsilon $\varepsilon$ or the Xi method which is based on a steepness threshold $\xi$ to identify clusters hierarchically. The Xi method is particularly good for identifying clusters with a varying density, whereas DBSCAN identifies clusters as the valleys below the given $\varepsilon$ value with separating hills. As the classes in this dataset have varying density the DBSCAN method, is most likely not a good choice for this dataset.

```{r 3.3.2 Optics-dbscan, warning=FALSE, echo=FALSE}
res_db <- extractDBSCAN(opt_clust, 1.55)
plot(res_db, sub = paste0("DBSCAN extraction eps = ", res_db$eps_cl))
```

Looking at the above reachability plot, it is clear that the DBSCAN method only finds 5 clusters, however it finds the most dense clusters. In the plot below the clusters are visualized as a scatterplot.

```{r 3.3.2 Optics-dbscan_2, warning=FALSE, echo=FALSE}
hullplot(dat, res_db, sub = paste0("DBSCAN extraction eps = ", res_db$eps_cl))

dat$db_clust <- as.factor(res_db$cluster)
dat$class <- compound.df$class
db_clust <- table(dat$db_clust, dat$class)

```

The mouth and the tounge are perfectly identified, whereas the right and left eye are missing some points and the rain is missing completely. Furthermore, the corona-virus includes some points from the rain, but is otherwise very nicely identified. The same pattern can be seen in the confusion matrix below. 

```{r 3.3.3 Optics-dbscan_3, warning=FALSE, echo=FALSE}
db_clust # 0 is noise points
#cluster_report(db_clust) # doesn't work as dbscan method only finds 4 clusters

```

As mentioned before, another method for extracting clusters from OPTICS is the Xi method, which utilizes a Steepness threshold $\xi$ to identify clusters hierarchically. This method might just be better at identifying all the clusters especially the rain, as the rain is quite different density-wise from the other clusters. 

```{r 3.3.4 Optics-xi, warning=FALSE, echo=FALSE}
res_xi <- extractXi(opt_clust, xi = 0.058)
plot(res_xi, sub = paste0("Xi extraction steepness threshold = ", res_xi$xi))

```

When looking at the above reachability plot, it looks like 6 clusters have been identified, however, there are still some points which are not identified, i.e., classified as noise. In the plot below the identified clusters are visualized on a scatter plot.

```{r 3.3.4 Optics-xi_2, warning=FALSE, echo=FALSE}
hullplot(dat[1:2], res_xi, sub = paste0("Xi extraction steepness threshold = ", res_xi$xi))

```

Looking at the above plot it is clear, that the mouth is still perfectly identified, whereas the tounge now is missing a point. More points have been identified of the left eye and maybe also the right eye. Furthermore, the corona-virus has some more points of the rain in it, but the rain is now also a cluster itself.

```{r 3.3.5 Optics-xi_3, warning=FALSE, echo=FALSE}
dat$xi_clust <- as.factor(res_xi$cluster)
xi_clust <- table(dat$xi_clust, dat$class)

xi_clust # 0 is noise points

cluster_report(xi_clust, cap = "OPTICS Xi-method")

```

Looking at the above confusion matrix and computed quality measures, it is clear that OPTICS paired with the Xi-method doesn't get all the points right, however, it is still very precise with almost all precision and recall scores above 0.8. The only class with a recall score below 0.8 is the rain, which with all of the clustering algorithms have been quite challenging to identify. 

### 3.4 DBSCAN
#### Test 1
<!-- Christian & Johannes: XX TODO: perform analysis -->
```{r 3.3.1 DBSCAN, warning=FALSE, echo=FALSE}
## Find suitable DBSCAN parameters:
## 1. We use minPts = dim + 1 = 5 for iris. A larger value can also be used.
## 2. We inspect the  k-NN distance plot for k = minPts - 1 = 4
kNNdistplot(td, k = 6-1)

## Noise seems to start around a 4-NN distance of .3
abline(h=.17, col = "red", lty=2)

# Fitting DBScan clustering Model 
# to training dataset
Dbscan_cl <- dbscan(td, eps = 0.17, MinPts = 6 + 1)
Dbscan_cl

# Checking cluster
Dbscan_cl$cluster

# Table
table(Dbscan_cl$cluster, compound.df$class)

# Plotting Cluster
plot(Dbscan_cl, td, main = "DBScan")

```
DBscan has two parameters: minPts (Dimensionality + 1, or higher), and epsilon from KKn.
We choose an epsilon from the Knn Distance plot (0.2) and use a minPts from the dimensionality 2 + 1 = 3

DBScan suggests 6 clusters.

"Lips", "Tongue", "Corona" are pretty good.

#### Test 2
```{r 3.3.1 DBSCAN_2, warning=FALSE, echo=FALSE}

kNNdistplot(td, k = 3-1)

abline(h=.17, col = "red", lty=2)

Dbscan_cl <- dbscan(td, eps = 0.4, MinPts = 3 + 1)
Dbscan_cl

Dbscan_cl$cluster

table(Dbscan_cl$cluster, compound.df$class)

plot(Dbscan_cl, td, main = "DBScan")


```

### 3.5 Silutte coefficient
<!-- : Johannes TODO: perform analysis -->
```{r}
set.seed(42069)

#Odd test , remove this.
fviz_nbclust(td, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")

#Just a test: 3 clusters
c_har_won3 <- kmeans(td, 3)
sil_res3 <- silhouette(c_har_won3$cluster, dist(norm.compound))
c_har_won3plot1 <- fviz_cluster(c_har_won3, data = td,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
c_har_won3plot2 <- fviz_silhouette(sil_res3)
grid.arrange(c_har_won3plot1, c_har_won3plot2, nrow =2, ncol = 1)

# 4 clusters
c_har_won4 <- kmeans(td, 4)
sil_res4 <- silhouette(c_har_won4$cluster, dist(norm.compound))
c_har_won4plot1 <- fviz_cluster(c_har_won4, data = td,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
c_har_won4plot2 <- fviz_silhouette(sil_res4)
grid.arrange(c_har_won4plot1, c_har_won4plot2, nrow =2, ncol = 1)

# 5 clusters
c_har_won5 <- kmeans(td, 5)
sil_res5 <- silhouette(c_har_won5$cluster, dist(norm.compound))
c_har_won5plot1 <- fviz_cluster(c_har_won5, data = td,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
c_har_won5plot2 <- fviz_silhouette(sil_res5)
grid.arrange(c_har_won5plot1, c_har_won5plot2, nrow =2, ncol = 1)

# 6 clusters
sil_res6 <- silhouette(c_har_won$cluster, dist(norm.compound))
c_har_wonplot1 <- fviz_cluster(c_har_won, data = td,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
c_har_wonplot2 <- fviz_silhouette(sil_res6)
grid.arrange(c_har_wonplot1, c_har_wonplot2, nrow =2, ncol = 1)

# 7 clusters
c_har_won7 <- kmeans(td, 7)
sil_res7 <- silhouette(c_har_won7$cluster, dist(norm.compound))
c_har_won7plot1 <- fviz_cluster(c_har_won7, data = td,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
c_har_won7plot2 <- fviz_silhouette(sil_res7)
grid.arrange(c_har_won7plot1, c_har_won7plot2, nrow =2, ncol = 1)

```







## 4 Results
<!-- XX TODO: Explain what we found / sum up results -->

### 4.1 Comparison



## 5 More intresing data
<!-- Max: XX TODO: find more intresting data for us and analyze it. -->
For the "More intresting data" we found IMDB review for the Shrek movies.
It includes stars (1-10), review, review title, movie name and date. 
We don't need the date, so it will be trimmed.
We also omit missing values, as it can't really be used.
Looking at the plots, any could be used, but none are easily clustered.

```{r 5.1.1 Movie data source, warning=FALSE, echo=FALSE, results='hide'}
# Source: https://github.com/raymondmyu/imdbreviews/tree/master/imdb_reviews
```

### 5.1 Data initiation
<!-- XX TODO: Initiate more interesting data -->
```{r 5.1.1 Review data initialiation, warning=FALSE, echo=FALSE}
# Source: https://github.com/raymondmyu/imdbreviews/tree/master/imdb_reviews
all.reviews <- read.csv("dataset/allreviews.csv", sep = ",") %>%
  dplyr::rename(review = text) %>% # Rename text to review
  mutate(review.and.title = paste(r_title, review)) %>%
  filter(title == c( # Filter for what movies to feature
                    "Shrek",
                    "Shrek 2",
                    "Shrek the Third",
                    "Shrek Forever After",
                    # "Toy Story 3",
                    # "Beauty and the Beast",
                    "The Killing of a Sacred Deer"
                  )
    ) %>%
  mutate(
    #date_date = as.Date(date), #Set date as date format, to do date filtering and stuff
    #sentiment_title = analyzeSentiment(r_title, removeStopwords = FALSE)[["SentimentGI"]],
     #    sentiment_reveiw = analyzeSentiment(review, reviewremoveStopwords = TRUE)[["SentimentGI"]],
         sentiment = analyzeSentiment(review.and.title, reviewremoveStopwords = TRUE)[["SentimentGI"]]
    ) %>%
  select(title, date, stars, sentiment
         # , sentiment_title, sentiment_reveiw
         ) %>% 
  na.omit() %>%
  mutate(
    date = as.Date(date) #Set date as date format, to do date filtering and stuff
  ) %>%
  group_by(title) %>% 
  arrange(title, date) %>% 
  top_n(31) #%>% #Equal amount of points - limited by Shrek 4
  # mutate(date = as.numeric(date))
# all.reviews %>% dplyr::filter(title == "The Killing of a Sacred Deer")

```

```{r 5.1.2 Average data table, warning=FALSE, echo=FALSE, dev='png'}
all.reviews.means <- all.reviews %>% 
  dplyr::group_by(title) %>% 
  mutate(
    date = as.Date(date) #Set date as date format, to do date filtering and stuff
  ) %>% 
  dplyr::summarise(stars = mean(stars),
            sentiment = mean(sentiment),
            date = mean(date)
            ) %>%
  mutate(stars = round(stars, 2), sentiment = round(sentiment, 5)) %>% 
  arrange(desc(stars))
  
all.reviews.means

```

```{r 5.1.3 Review data Vizualization, warning=FALSE, echo=FALSE}
stars_sen <- all.reviews %>% # stars vs sentiment review
  select(title, stars, sentiment) %>%
  dplyr::rename("x" = stars,
         "y" = sentiment,
         "class" = title)
ggplot(stars_sen, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Stars and sentiment") +
  theme_light()

date_sen <- all.reviews %>% # stars vs sentiment review
  select(title, date, sentiment) %>%
  dplyr::rename("x" = date,
         "y" = sentiment,
         "class" = title)
ggplot(date_sen, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Date and sentiment") +
  theme_light()

date_stars <- all.reviews %>% # stars vs sentiment review
  select(title, date, stars) %>%
  dplyr::rename("x" = date,
         "y" = stars,
         "class" = title)
ggplot(date_stars, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Date and Stars") +
  theme_light()

```

```{r 5.1.5 Focus data for 5, warning=FALSE, echo=FALSE}
data <- all.reviews %>% # stars vs sentiment review
  select(date, sentiment) %>%
  dplyr::rename("class" = title)# %>% 
  # mutate(date = as.numeric(date))

data.points <- all.reviews %>% # stars vs sentiment review
  ungroup() %>% 
  select(date, sentiment) %>%
  mutate(date = as.numeric(date))

classNames <- all.reviews %>% # stars vs sentiment review
  select(title) %>% 
  .$title

unique_class <- classNames %>% 
  unique()

```

### 5.2 Analysis

### 5.3 k-means
With k-means we hoped it would be able to categorize
the movies with time and sentiment. We do expect some of the reviews
to be mislabeled. Especially Shrek.

```{r 5.3.1 K-means testing methods, warning=FALSE, echo=FALSE}
set.seed(42069) # Set seed to remove randomness when replicating results
c_macqueen_shrek <- kmeans(data.points, 5, iter.max = 20, algorithm = "MacQueen")
c_forgy_shrek <- kmeans(data.points, 5, iter.max = 20, algorithm = "Forgy")
c_har_won_shrek <- kmeans(data.points, 5)

test_shrek <- data.frame(macqueen = c_macqueen_shrek$cluster,
                   forgy = c_forgy_shrek$cluster,
                   har_won = c_har_won_shrek$cluster,
                   class = classNames)

macqueen_shrek <- table(test_shrek$macqueen, test_shrek$class, dnn = c("MacQueen", "Class"))
forgy_shrek <- table(test_shrek$forgy, test_shrek$class, dnn = c("Forgy", "Class"))
har_won_shrek <- table(test_shrek$har_won, test_shrek$class, dnn = c("Harting-Wong", "Class"))

macqueen_shrek
forgy_shrek
har_won_shrek

cluster_report(macqueen_shrek, cap = "MacQueen")
cluster_report(forgy_shrek, cap = "Forgy")
cluster_report(har_won_shrek, cap = "Hartigan Wong")

si_shrek <- silhouette(c_har_won_shrek$cluster, dist(data.points))
silhouette_score <- mean(si_shrek[,3])

```

```{r 5.3.2 k-means-visualization, warning=FALSE, echo=FALSE}
data$macqueen <- factor(test_shrek$macqueen)
data$forgy <- factor(test_shrek$forgy)
data$har_won <- factor(test_shrek$har_won)

plot_stuff <- function(df, predicted, cap) {
  ggplot(df, aes(date, sentiment, shape = class, color = predicted)) +
    geom_point() +
    labs(title = cap) +
    theme_light()
}


plot_stuff(data, data$macqueen, "Macqueen")
plot_stuff(data, data$forgy, "Forgy")
plot_stuff(data, data$har_won, "Hartigan Wong")

```
We were right. Forgy does a damn good job.
Hartigan Wong and Macqueen mixes shrek 1 and 2 up, and thinks shrek 4 is 2 movies.

### 5.4 knn


### 5.5 OPTICS
```{r 5.5.1 Optics-setup, warning=FALSE, echo=FALSE}
set.seed(7010) # YOLO
# data already set as data

opt_clust_shrek <- optics(data.points, minPts = 20)
plot(opt_clust_shrek, sub = "minPts = 20")

```

When looking at the "valleys" in the above reachability plot, it seems as if there are 5 classes with somewhat alike density.

Comparing the reachability plot to the clustering order plot below, we can see that the clustering happens around the densest parts of the clusters.

```{r 5.5.1 Optics-order, warning=FALSE, echo=FALSE}
plot(data.points, col = "grey", main = "Clustering order", sub = "minPts = 20")
polygon(data.points[opt_clust_shrek$order,])

```


```{r 5.5.2 Optics-dbscan, warning=FALSE, echo=FALSE}
res_db_shrek <- extractDBSCAN(opt_clust_shrek, 250)
plot(res_db_shrek, sub = paste0("DBSCAN extraction eps = ", res_db_shrek$eps_cl))

```

Looking at the above reachability plot, it is clear that the DBSCAN method only finds 5 clusters, however it finds the most dense clusters. In the plot below the clusters are visualized as a scatterplot.
<!-- XX TODO: rewrite statement and dix the remodeling that happens for no reason now? -->

```{r 5.5.2 Optics-dbscan_2, warning=FALSE, echo=FALSE}
hullplot(data.points, res_db_shrek, sub = paste0("DBSCAN extraction eps = ", res_db_shrek$eps_cl))

data.points_db_clust <- as.factor(res_db_shrek$cluster)
db_clust_shrek <- table(data.points_db_clust, classNames)

```
It does do a good job, but shrek 1 seems to be more limited than the others.
 

```{r 5.5.3 Optics-dbscan_3, warning=FALSE, echo=FALSE}
db_clust_shrek # 0 is noise points
cluster_report(db_clust_shrek, cap = "DBSCAN extraction")

```

Xi method:

```{r 5.5.4 Optics-xi, warning=FALSE, echo=FALSE}
res_xi_shrek <- extractXi(opt_clust_shrek, xi = 0.225)
plot(res_xi_shrek, sub = paste0("Xi extraction steepness threshold = ", res_xi_shrek$xi))

```

When looking at the above reachability plot, it looks like most movies were identified, however, shrek 2 seems to be overlapping with shrek- 2 and 3.

In the plot below the identified clusters are visualized on a scatter plot.

```{r 5.5.4 Optics-xi_2, warning=FALSE, echo=FALSE}
hullplot(data.points, res_xi_shrek, sub = paste0("Xi extraction steepness threshold = ", res_xi_shrek$xi))

```

Looking at the above plot, The movies do create some meaningful clusters, but it does have some trouble with Shrek 2, as well as some points that overlap in other clusters. 

```{r 5.5.5 Optics-xi_3, warning=FALSE, echo=FALSE}
data_xi_clust <- as.factor(res_xi_shrek$cluster)
xi_clust_shrek <- table(data_xi_clust, classNames)

xi_clust_shrek # 0 is noise points

cluster_report(xi_clust_shrek, cap = "OPTICS Xi-method")

```

Looking at the above confusion matrix and computed quality measures, it is clear that OPTICS paired with the Xi-method doesn't get all the points right, however, it is still very precise. 
<!-- XX TODO: Fix text -->

### 5.6 DBSCAN
#### Test 1
```{r 5.6.1 DBSCAN test 1, warning=FALSE, echo=FALSE}
## Find suitable DBSCAN parameters:
## 1. We use minPts = dim + 1 = 5 for iris. A larger value can also be used.
## 2. We inspect the  k-NN distance plot for k = minPts - 1 = 4
kNNdistplot(data.points, k = 5)

## Noise seems to start around a 4-NN distance of .3
abline(h=.17, col = "red", lty=2)

# Fitting DBScan clustering Model 
# to training dataset
Dbscan_cl_shrek <- dbscan(data.points, eps = 27, MinPts = 4)
Dbscan_cl_shrek

# Checking cluster
Dbscan_cl_shrek$cluster

# Table
table(Dbscan_cl_shrek$cluster, classNames)

# Plotting Cluster
plot(Dbscan_cl_shrek, data.points, main = "DBScan")

```
#### Test 2
```{r 5.6.2 DBSCAN test 2, warning=FALSE, echo=FALSE}
## Find suitable DBSCAN parameters:
## 1. We use minPts = dim + 1 = 5 for iris. A larger value can also be used.
## 2. We inspect the  k-NN distance plot for k = minPts - 1 = 4
kNNdistplot(data.points, k = 5)

## Noise seems to start around a 4-NN distance of .3
abline(h=.17, col = "red", lty=2)

# Fitting DBScan clustering Model 
# to training dataset
Dbscan_cl_shrek <- dbscan(data.points, eps = 350, MinPts = 15)
Dbscan_cl_shrek

# Checking cluster
Dbscan_cl_shrek$cluster

# Table
table(Dbscan_cl_shrek$cluster, classNames)

# Plotting Cluster
plot(Dbscan_cl_shrek, data.points, main = "DBScan")

```
DBscan has two parameters: minPts (Dimensionality + 1, or higher), and epsilon from KKn.
We choose an epsilon from the Knn Distance plot (0.2) and use a minPts from the dimensionality 2 + 1 = 3

DBScan suggests 6 clusters.

"Lips", "Tongue", "Corona" are pretty good.



### 5.7 Silutte coefficient


### 6 Results
<!-- XX TODO: Explain what we found / sum up results -->

### 7 Clustering results
<!-- XX Answer: What can you learn about that [new] data by using clustering? -->


```{r name1, warning=FALSE, echo=FALSE}

```

