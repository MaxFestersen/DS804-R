---
title: 'DS804: Exercise 11: Group Project 1: Clustering'
author: Alexander Dernild, Christian R. Andersen, Johannes Sørensen, Mathias P.
  Larsen & Max F. Hansen
date: "28/4/2021"
output: pdf_document
---
```{r Imports, echo = FALSE, message = FALSE}
# Imports ---------------------------------------
library(tidyverse) # For convenience, plots, tables and probably some other stuff.
library(cluster)
library(factoextra)
library(SentimentAnalysis) # for sentiment analysis
library(dbscan)
library(class)
library(caret)
library(plyr)
library(fpc)
library(e1071)
# Source ----------------------------------------
source("../cluster_quality.R") # Functions for data analysis
```

```{r 0.0.1 Seeding, echo = FALSE, message = FALSE}
set.seed(42069) # Set seed to remove randomness when replicating results
```

# **DS804 Assignment 1 - Clustering**
28/04/2021  

## Created by  

* Alexander Ibsen Dernild, alder17  
* Christian Rohde Andersen, chran20  
* Johannes Christian Sørensen, josoe20  
* Mathias Profft Larsen, matla20  
* Max Festersen Hansen, maxfh20  

## 1. Introduction
For this project, we have chosen a dataset in the "Shape sets" at [https://cs.joensuu.fi/sipu/datasets/]. In this report we will explain why we chose this dataset, which methods we have chosen to analyze the data and explain the results of our analysis. At last we will be making a comparison of the methods and see how it can be upscaled to larger datasets.


### 1.1 Zahn's Compound dataset
For this exercise, we needed shape data set. We choose to use the compound
dataset, from C.T. Zahn, **IEEE Transactions on Computers**, 1971.
The compound data was chosen for... reasons.<!-- XX TODO: expand reason.  -->

```{r 1.1.1 Loading-dataset, echo = FALSE, message = FALSE}
# Data load
compound.df <- read.csv("Compound.txt", sep = "\t", header = F)
# Adding lables
names(compound.df) <- c("x", "y", "class")

compound.df <- compound.df %>% 
  mutate(class = ifelse(class == 1, "Rain", class)) %>% 
  mutate(class = ifelse(class == 2, "Corona-virus", class)) %>% 
  mutate(class = ifelse(class == 3, "Left-eye", class)) %>% 
  mutate(class = ifelse(class == 4, "Right-eye", class)) %>% 
  mutate(class = ifelse(class == 5, "Lips", class)) %>% 
  mutate(class = ifelse(class == 6, "Tounge", class))

compound.df$class <- factor(compound.df$class)
```

```{r 1.1.2 Plotting-initial-data, warning=FALSE, echo=FALSE}
ggplot(compound.df, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Compound Data") +
  theme_light()
```
Above the data can be seen, colored by class. The clases is named 1-8.
The data had 8 classes, thus 8 colors are used.


```{r 1.1.3 Normalizing-data, warning=FALSE, echo=FALSE}
norm.compound <- scale(compound.df[1:2]) %>% 
  data.frame()
norm.compound$class <- compound.df$class
td <- norm.compound[1:2] # Test data - the data we test on

ggplot(norm.compound, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Compound Data normalized") +
  theme_light()


fviz_nbclust(norm.compound[1:2], kmeans, method='silhouette') # elbow plot
```
We normalize the data to be closer to zero (centered zero). This is done to normalize the weight and remove bias from large numbers.

## 2 Methods  


### 2.1 Algorithms and implementation  
<!-- XX TODO: explain witch algorithms and implementations that were used -->



## 3 Analysis of the Methods
<!-- XX TODO: find analysis methods -->


### 3.1 k-means
With k-means we expected it would be able to create good clusters
for the eyes and tongue, since they have a clear mean. We would expect it to
have some trouble with the lips, the rain and the virus,
since the cluster-means overlap with other clusters.

```{r 3.1.1 K-means testing methods, warning=FALSE, echo=FALSE}
set.seed(42069) # Set seed to remove randomness when replicating results
c_macqueen <- kmeans(td, 6, iter.max = 20, algorithm = "MacQueen")
c_forgy <- kmeans(td, 6, iter.max = 20, algorithm = "Forgy")
c_har_won <- kmeans(td, 6)

test <- data.frame(macqueen = c_macqueen$cluster,
                   forgy = c_forgy$cluster,
                   har_won = c_har_won$cluster,
                   class = norm.compound$class)

macqueen <- table(test$macqueen, test$class, dnn = c("MacQueen", "Class"))
forgy <- table(test$forgy, test$class, dnn = c("Forgy", "Class"))
har_won <- table(test$har_won, test$class, dnn = c("Harting-Wong", "Class"))

macqueen
forgy
har_won

cluster_report(macqueen)
cluster_report(forgy)
cluster_report(har_won)

si <- silhouette(c_har_won$cluster, dist(norm.compound))
silhouette_score <- mean(si[,3])
```

```{r 3.1.2 k-means-visualization, warning=FALSE, echo=FALSE}
norm.compound$macqueen <- factor(test$macqueen)
norm.compound$forgy <- factor(test$forgy)
norm.compound$har_won <- factor(test$har_won)

plot_stuff <- function(df, predicted, cap) {
  ggplot(df, aes(x, y, shape = class, color = predicted)) +
    geom_point() +
    labs(title = cap) +
    theme_light()
}

plot_stuff(norm.compound, norm.compound$macqueen, "Macqueen")
plot_stuff(norm.compound, norm.compound$forgy, "Forgy")
plot_stuff(norm.compound, norm.compound$har_won, "Hartigan Wong")
```
Using the k-means algorithms: Macqueen, Loyd-Forgy and Hartigan-Wong. We found
Hartigan-Wong was the most accurate when looking at the accuracy of estimates,
and mean macro average. They are visualized above.
Regardless of method, it seems the algorithms are having a hard time distinguishing:
1. Lips & Tongue
2. Left-eye & Right-eye
3. Rain
4. Corona-virus
5. Corona-virus & Rain

It is worth noting that Hartigan-Wong gets Right-eye more wrong with no clustering,
and has a harder time handling Rain and Corona-Virus,
using more clusteres there than the others. It does get Lips and Left-eye right.

It has the best visual result, with the creation og a Mouth, Eyes and multicolored
Scenery.

From what we predicted, we correctly predicted some of the issues with kmeans.
Additionally we were surprised to see Tongue being split in some cases.
The eyes were also never classified correctly, with them being clustered close
together and sometimes together with Lips and Tongue.
<!--XX TODO: Ret fejl i de beskrivelser Max har skrevet ovenfor -->

### 3.2 knn
<!-- Mathias: XX TODO: perform analysis -->
<!-- Mathias: XX TODO: perform analysis -->
to use KNN we need to have noise free dataset and all attributes of the dataset need to belong to a class.
we start by choosing the value of K we square root the total number of data points available in the dataset, generally it is a good idea for k to be a odd number.

```{r 3.2.1 KNN preproccesing, warning=FALSE, echo=FALSE}
set.seed(42069)
#getting data ready
ran <- sample(1:nrow(compound.df), 0.80* nrow(compound.df))
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
knn_compound_norm <- as.data.frame(lapply(compound.df[,c(1,2)], nor))

#creating a normalized dataset with classes(for Cross validation)
norm.test <- cbind(knn_compound_norm, compound.df$class)
names(norm.test) <- c("x", "y", "class")


#using all data, test and training are the same
compound_train <- knn_compound_norm #all of data
compound_test <- knn_compound_norm #all of data
compound_target_category <- compound.df[,3]#all data
compound_test_category <- compound.df[,3]#all data

#using acutally training data and test data
compound_train <- knn_compound_norm[ran,] #train data values
compound_test <- knn_compound_norm[-ran,] #test data values
compound_target_category <- compound.df[ran,3] #train data category
compound_test_category <- compound.df[-ran,3] #test data category




#when K=1: potential underfit
#when K approaches total number of rows:potential overfit
# K should be an odd number
#so we should cross validate
sqrt(399)
#KNN algorithm where key is equal to square root of total number of rows
predictions <- knn(compound_train,compound_test,cl=compound_target_category,use.all = TRUE ,k=4)
```
If we use the whole dataset as a training and test set, and also set K to 1. The the predictions will always be 100% correct, because we have overfitted the model to the exact same data as we test on.
We can solve this issue be using a training and a test set that together form the whole dataset, but another problem then arise.

```{r 3.2.2 KNN confussion matrix/predictions, warning=FALSE, echo=FALSE}
#confussion matrix
tab <- table(predictions,compound_test_category)
#tab

#accuracy of test set/model
#accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

#predictions
predictions

#confusion Matrix with accuracy
confusionMatrix(predictions ,compound_test_category)

#CrossTable
library(gmodels)
CrossTable(y=compound_test_category,x=predictions,prop.chiq=FALSE)


```

```{r 3.2.3 KNN optimal k based on test/training set, warning=FALSE, echo=FALSE}
#displaying the accuracy of the KNN algorithm given the K=i
i=1
k.optm=1
for (i in 1:40){
knn.mod <- knn(train=compound_train, test=compound_test,cl=compound_target_category,k=i)
k.optm[i] <- 100* sum(compound_test_category==knn.mod)/NROW(compound_test_category)
k=i
cat(k, '=',k.optm[i],'\n')
}

plot(k.optm, type="b", xlab="K-Value", ylab="Accuracy level")
```
When we want to set a new K we need to remember that as the complexity of the KNNmodel, is determined by the value of K(lower value= more complex). So training accuracy rises as the complexity increases(lower k=higher training accuracy).
so testing accuracy is a more appropriate way to measure the quality, as it penalizes models that are too complex or not complex enough

So when we maximize the training accuracy, we have a high chance of creating a to complex model, so the model ends up being overfitted to the data and not generalize enough.

```{r 3.2.4 KNN plot based test/training set, warning=FALSE, echo=FALSE}
#plot
plot.df = data.frame(compound_test, predicted = predictions)
plot.df1 = data.frame(x = plot.df$y, 
                      y = plot.df$x, 
                      predicted = plot.df$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]

boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)

border_plot<-ggplot(plot.df, aes(y, x, color = predicted, fill = predicted)) +
  geom_point(size = 2)
#turn the graph so it is facing the right way
border_plot<-border_plot+ coord_flip()
#plot without border
border_plot
#plot with border
border_plot+geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
```

Wee can use cross validation to get a more optimal k, by using the whole data set and split it up to several pieces, this way our training set will be larger, and we avoid the disadvantage of testing our whole data set in one go.
```{r 3.2.5 KNN 10 fold cross validation, warning=FALSE, echo=FALSE}
set.seed(41040)
trControl <- trainControl(method  = "cv",
                          number  = 10)





cross_fit <- train(class ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 3:30),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = norm.test)
#optimal K based on average accuracy
cross_fit
confusionMatrix(cross_fit)
#Cross validation best K as a plot
plot(cross_fit)

#Cross validation prediction tested on the dataset
knnPredict <- predict(cross_fit,newdata = compound_test )
confusionMatrix(knnPredict,compound.df$class)
#Get the confusion matrix to see accuracy value and other parameter values
#plot of ROC(repeated Cross-validation)
plot(cross_fit, print.thres = 0.5, type="S")
```



### 3.3 OPTICS
<!-- Alexander: XX TODO: perform analysis -->

```{r 3.3.1 Optics, warning=FALSE, echo=FALSE}
set.seed(2)
dat <- compound.df[1:2] # data

opt_clust <- optics(dat, minPts = 7)
plot(opt_clust, sub = "minPts = 7")

plot(dat, col = "grey", main = "Clustering order", sub = "minPts = 7")
polygon(dat[opt_clust$order,])
```


```{r 3.3.2 Optics, warning=FALSE, echo=FALSE}
res_db <- extractDBSCAN(opt_clust, 1.6)
plot(res_db, sub = paste0("DBSCAN extraction eps = ", res_db$eps_cl))
hullplot(dat, res_db, sub = paste0("DBSCAN extraction eps = ", res_db$eps_cl))

dat$db_clust <- as.factor(res_db$cluster)
dat$class <- compound.df$class
db_clust <- table(dat$db_clust, dat$class)
```

DBSCAN extraction only finds 4 clusters, most likely because the classes has a variable density. 

```{r 3.3.3 Optics, warning=FALSE, echo=FALSE}
db_clust # 0 is noise points
#cluster_report(db_clust) # doesn't work as dbscan method only finds 4 clusters
```


Another method for extracting clusters from OPTICS is the Xi method, which utilizes a Steepness threshold to identify clusters hierarchically. 

```{r 3.3.4 Optics, warning=FALSE, echo=FALSE}
res_xi <- extractXi(opt_clust, xi = 0.058)
plot(res_xi, sub = paste0("Xi extraction steepness threshold = ", res_xi$xi))
hullplot(dat[1:2], res_xi, sub = paste0("Xi extraction steepness threshold = ", res_xi$xi))
```

As seen in the above plot the Xi extraction is much better at finding the clusters.

```{r 3.3.5 Optics, warning=FALSE, echo=FALSE}
dat$xi_clust <- as.factor(res_xi$cluster)
xi_clust <- table(dat$xi_clust, dat$class)

xi_clust # 0 is noise points
cluster_report(xi_clust)
```

### 3.4 DBSCAN
#### Test 1
<!-- Christian & Johannes: XX TODO: perform analysis -->
```{r 3.3.1 DBSCAN, warning=FALSE, echo=FALSE}
## Find suitable DBSCAN parameters:
## 1. We use minPts = dim + 1 = 5 for iris. A larger value can also be used.
## 2. We inspect the  k-NN distance plot for k = minPts - 1 = 4
kNNdistplot(td, k = 6-1)

## Noise seems to start around a 4-NN distance of .3
abline(h=.17, col = "red", lty=2)

# Fitting DBScan clustering Model 
# to training dataset
Dbscan_cl <- dbscan(td, eps = 0.17, MinPts = 6 + 1)
Dbscan_cl

# Checking cluster
Dbscan_cl$cluster

# Table
table(Dbscan_cl$cluster, compound.df$class)

# Plotting Cluster
plot(Dbscan_cl, td, main = "DBScan")


```
DBscan has two parameters: minPts (Dimensionality + 1, or higher), and epsilon from KKn.
We choose an epsilon from the Knn Distance plot (0.2) and use a minPts from the dimensionality 2 + 1 = 3

DBScan suggests 6 clusters.

"Lips", "Tongue", "Corona" are pretty good.

#### Test 2
```{r 3.3.1 DBSCAN, warning=FALSE, echo=FALSE}

kNNdistplot(td, k = 6-1)

abline(h=.17, col = "red", lty=2)

Dbscan_cl <- dbscan(td, eps = 0.17, MinPts = 6 + 1)
Dbscan_cl

Dbscan_cl$cluster

table(Dbscan_cl$cluster, compound.df$class)

plot(Dbscan_cl, td, main = "DBScan")


```

### 3.5 Silutte coefficient
<!-- : XX TODO: perform analysis -->




## 4 Results
<!-- XX TODO: Explain what we found / sum up results -->

### 4.1 Comparison



## 5 More intresing data
<!-- Max: XX TODO: find more intresting data for us and analyze it. -->
For the "More intresting data" we found IMDB review for the Shrek movies.
It includes stars (1-10), review, review title, movie name and date. 
We don't need the date, so it will be trimmed.
We also omit missing values, as it can't really be used.
```{r 5.0.1 Review data initialiation, warning=FALSE, echo=FALSE}
# Source: https://github.com/raymondmyu/imdbreviews/tree/master/imdb_reviews
all.reviews <- read.csv("dataset/allreviews.csv", sep = ",") %>%
  # select(-date) %>% # remove date from dataset
  dplyr::rename(review = text) %>% # Rename text to review
  mutate(review.and.title = paste(r_title, review)) %>%
  filter(title == c( # Filter for what movies to feature
                    "Shrek",
                    "Shrek 2",
                    "Shrek the Third",
                    "Shrek Forever After",
                    # "Toy Story 3",
                    # "Beauty and the Beast",
                    "The Killing of a Sacred Deer"
                  )
    ) %>%
  mutate(
    #date_date = as.Date(date), #Set date as date format, to do date filtering and stuff
    #sentiment_title = analyzeSentiment(r_title, removeStopwords = FALSE)[["SentimentGI"]],
     #    sentiment_reveiw = analyzeSentiment(review, reviewremoveStopwords = TRUE)[["SentimentGI"]],
         sentiment = analyzeSentiment(review.and.title, reviewremoveStopwords = TRUE)[["SentimentGI"]]
    ) %>%
  select(title, date, stars, sentiment
         # , sentiment_title, sentiment_reveiw
         ) %>% 
  na.omit() %>%
  group_by(title) %>% 
  arrange(date) %>% 
  top_n(50) #%>% 
  # mutate(date = as.numeric(date))

all.reviews.means <- all.reviews %>% 
  group_by(title) %>% 
  summarise(stars = mean(stars),
            # sentiment = mean(sentiment),
            date = mean(date)
            # sentiment_title = mean(sentiment_title),
            # sentiment_reveiw = mean(sentiment_reveiw)
            ) %>% 
  arrange(stars)
```

```{r 5.0.2 Review data Vizualization, warning=FALSE, echo=FALSE}
stars_sen <- all.reviews %>% # stars vs sentiment review
  select(title, stars, sentiment) %>%
  dplyr::rename("x" = stars,
         "y" = sentiment,
         "class" = title)
ggplot(stars_sen, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Stars and sentiment") +
  theme_light()

date_sen <- all.reviews %>% # stars vs sentiment review
  select(title, date, sentiment) %>%
  dplyr::rename("x" = date,
         "y" = sentiment,
         "class" = title)
ggplot(date_sen, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Date and sentiment") +
  theme_light()

date_stars <- all.reviews %>% # stars vs sentiment review
  select(title, date, stars) %>%
  dplyr::rename("x" = date,
         "y" = stars,
         "class" = title)
ggplot(date_stars, aes(x, y, shape = class, color = class)) +
  geom_point() +
  labs(title = "Date and Stars") +
  theme_light()
```
Looking at the plots, any could be used, but none are easily clustered.

```{r 5.0.3 Unused shrek vizualizations, warning=FALSE, echo=FALSE}
# shrek.1 <- read.csv("dataset/Shrek.csv", sep = ",")[-1]
# shrek.2 <- read.csv("dataset/Shrek 2.csv", sep = ",")[-1]
# shrek.3 <- read.csv("dataset/Shrek the Third.csv", sep = ",")[-1]
# shrek.4 <- read.csv("dataset/Shrek Forever After.csv", sep = ",")[-1]
# shreks <- shrek.1 %>%
#   full_join(shrek.2) %>% 
#   full_join(shrek.3) %>% 
#   full_join(shrek.4) %>%
#   select(-date) %>% # remove date from dataset
#   rename("review" = text) %>% # Rename text to review
#   mutate(review.and.title = paste(r_title, review)) %>% 
#   mutate(sentiment_title = analyzeSentiment(r_title, removeStopwords = FALSE)[["SentimentGI"]],
#          sentiment_reveiw = analyzeSentiment(review, reviewremoveStopwords = TRUE)[["SentimentGI"]],
#          sentiment = analyzeSentiment(review.and.title, reviewremoveStopwords = TRUE)[["SentimentGI"]]) %>%
#   na.omit() %>%   # Remove any rows with na. Because fuck 'em.
#   rowwise() %>%
#   mutate(avg_sentiment = mean(c(sentiment_title, sentiment_reveiw)))
# # shreks.sentiment.r_title <- analyzeSentiment(shreks$r_title, # Analyse sentiment on title
# #                                              removeStopwords = FALSE) # Do not remove stopwords, as titles can be short, which leads to na values.
# # shreks.sentiment.review <- analyzeSentiment(shreks$review,
# #                                             removeStopwords = TRUE) # Revmove stopswords.
# 
# Shreksiments <- shreks %>% # sentiment of title vs review
#   select(title, sentiment_title, sentiment_reveiw) %>% 
#   rename("x" = sentiment_title,
#          "y" = sentiment_reveiw,
#          "class" = title)
# ggplot(Shreksiments, aes(x, y, shape = class, color = class)) +
#   geom_point() +
#   labs(title = "Shrek sentiment on title and review") +
#   theme_light()
# 
# Shrek_stars_title_sen <- shreks %>% # stars vs sentiment title
#   select(title, stars, sentiment_title) %>% 
#   rename("x" = stars,
#          "y" = sentiment_title,
#          "class" = title)
# ggplot(Shrek_stars_title_sen, aes(x, y, shape = class, color = class)) +
#   geom_point() +
#   labs(title = "Shrek stars and title sentiment") +
#   theme_light()
# 
# Shrek_stars_review_sen <- shreks %>% # stars vs sentiment review
#   select(title, stars, sentiment_reveiw) %>% 
#   rename("x" = stars,
#          "y" = sentiment_reveiw,
#          "class" = title)
# ggplot(Shrek_stars_review_sen, aes(x, y, shape = class, color = class)) +
#   geom_point() +
#   labs(title = "Shrek stars and review sentiment") +
#   theme_light()
# 
# Shrek_stars_avg_sen <- shreks %>% # stars vs sentiment review
#   select(title, stars, avg_sentiment) %>% 
#   rename("x" = stars,
#          "y" = avg_sentiment,
#          "class" = title)
# ggplot(Shrek_stars_avg_sen, aes(x, y, shape = class, color = class)) +
#   geom_point() +
#   labs(title = "Shrek stars and title and review average sentiment") +
#   theme_light()
# 
# Shrek_stars_complete_sen <- shreks %>% # stars vs sentiment review
#   select(title, stars, sentiment_reveiw) %>% 
#   rename("x" = stars,
#          "y" = sentiment_reveiw,
#          "class" = title)
# ggplot(Shrek_stars_complete_sen, aes(x, y, shape = class, color = class)) +
#   geom_point() +
#   labs(title = "Shrek stars and complete sentiment") +
#   theme_light()

```

### 5.1 Data initiation
<!-- XX TODO: Initiate more intersting data -->


### 5.2 Methods  


### 5.3 Algorithms and implementation  
<!-- XX TODO: explain witch algorithms and implementations that were used -->

  
### 5.4 Analysis
<!-- XX TODO: perform analysis -->


### 5.5 Results
<!-- XX TODO: Explain what we found / sum up results -->


### 6 Clustering results
<!-- XX Answer: What can you learn about that [new] data by using clustering? -->


```{r name1, warning=FALSE, echo=FALSE}

```

